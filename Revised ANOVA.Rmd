---
title: "Revised Regression"
author: "Evan Dragich"
date: "Last compiled on `r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
---

```{r include = FALSE}
knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE)
```


# Data Cleaning

Check out the .Rmd file for full code (omitted here for brevity).

```{r load-packages-data, echo = FALSE}
library(readxl) # tidy import of xlsx
library(tidyverse) # dplyr, magrittr
library(broom) # tidy
library(lme4) # lmer() (linear mixed models)
library(lmerTest) # lmer() anova
library(emmeans) # lmer() post hoc comparisons


# clean individual datasets into tidy format
mass_2019 <- read_excel(here::here("data_mass_only_from_2019_folder.xlsx"),
  sheet = "mass balances"
) %>%
  dplyr::select(-c(...2, ...3)) %>%
  drop_na(...1) %>%
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 12))) %>%
  rename(blooms_level = `...1`) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(
    year = "2019",
    student_id = str_to_lower(student_id)
  ) # 2019 student ids will be lowercase; 2020 uppercase

kinetics_2019 <- read_excel(here::here("data_kinetics_from_2019_folder.xlsx"),
  sheet = "kinetics"
) %>%
  dplyr::select(-c(...2, ...3)) %>%
  drop_na(...1) %>%
  slice_head(n = 24) %>% # remove irrelevant PBL1 data
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 6))) %>%
  rename(blooms_level = `...1`) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(
    year = "2019",
    student_id = str_to_lower(student_id)
  )

# 2020 didnt come with student id letter codes, so I'm making them here
# same sequence as 2019, but all caps to distinguish
helper_letters <- c(LETTERS, paste0(LETTERS, LETTERS)) %>%
  .[1:48]

mass_2020 <- read_excel(here::here("combined_2020.xlsx"),
  sheet = "Mass"
)

names(mass_2020) <- c("blooms_level", helper_letters)

mass_2020 <- mass_2020 %>%
  drop_na(A) %>%
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 12))) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(year = "2020",
         blooms_level = str_to_lower(blooms_level))

kinetics_2020 <- read_excel(here::here("combined_2020.xlsx"),
  sheet = "Kinetics"
)

names(kinetics_2020) <- c("blooms_level", helper_letters)

kinetics_2020 <- kinetics_2020 %>%
  drop_na(A) %>%
  slice(c(1:18, 25:30)) %>% # remove irrelevant PBL1 data
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 6))) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(year = "2020",
         blooms_level = str_to_lower(blooms_level))

```

```{r combine-years, echo = FALSE}
# create final dataframes for each domain
mass <- rbind(mass_2019, mass_2020) %>%
  mutate(
    student_id = as.factor(student_id),
    year = factor(year),
    blooms_level = factor(blooms_level, 
                          levels = c("remember", "understand", "analyze", 
                                     "apply", "evaluate", "create")),
    task = factor(task, levels = c("lec", "icp", "hw", "pbl"))
  )

kinetics <- rbind(kinetics_2019, kinetics_2020) %>%
  mutate(
    student_id = factor(student_id),
    year = factor(year),
    blooms_level = factor(blooms_level, 
                          levels = c("remember", "understand", "analyze", 
                                     "apply", "evaluate", "create")),
    task = factor(task, levels = c("lec", "icp", "hw", "pbl"))
  )
```

# Motivation for a new statistical method

My personal biggest concern with our methods, even before receiving reviewer comments, was the blatant violation of the [independence condition](https://statistics.laerd.com/spss-tutorials/multiple-regression-using-spss-statistics.php) for standard linear regression. I **knew** there was an alternative--something that could account for the repeated measures, or "nesting" of `blooms_level * task` within each student.

During my search, I finally found it-- [a two-way repeated measures ANOVA](https://statistics.laerd.com/spss-tutorials/two-way-repeated-measures-anova-using-spss-statistics.php). This page has good vignettes for sample scenarios, which you can draw analogies to our data structure.

I tried to find out how to implement it, and ran into [recent literature](https://www.r-bloggers.com/2018/04/how-to-do-repeated-measures-anovas-in-r/) eschewing ANOVA in favor of regression. This page also pointed me to the family of linear mixed models, which extend simple linear regression by allowing for random effects, in addtion to fixed.

## Random Effects

Random effects control for the repeated measures factor, by essentially allowing factors to vary along `student_id` without using up degrees of freedom to try to make sense of the results. In other words, `student_id` is a random effect in our case because we have to account for each student having a different, random intercept, before examining the global, fixed effects of `task` and `blooms_level`.

The `(1 | year/student_id)` term in these models indicates that we have to account for the random, nuisance variance caused by each student and each year/cohort before examining the effects of `blooms_level`, `task`, and their interaction. Specifically, the `/` nesting operator means that we have a first random intercept to account for variance across years, and after accounting for this variance we introduce a random intercept to further account for that particular student's variance.

# Mass Analysis

## Omnibus ANOVA

```{r mass-lmm}
mass_lmm <- lmer(rating ~ blooms_level * task + (1 | year/student_id), data = mass)

anova(mass_lmm) %>%
  tidy()
```

Great! Significant main effects for `task` and `blooms_level`, as well as their interaction. The next step is to break this down into pairwise comparisons. I chose to analyze pairs of `blooms_level` within each `task`; this is most analogous to our former method.

## Marginal Means and Contrasts

``` {r mass-emmeans}
mass_emm <- ref_grid(mass_lmm) %>%
  emmeans(specs = pairwise ~ blooms_level | task)

tidy(mass_emm$emmeans)
mass_emm$contrasts %>%
  summary(infer = TRUE)
```

(Surprisingly, there don't exist any simple packages to transform this output into a neater table--I think we'd have to do it ourselves.)

The first table is the estimated marginal means [(EMM)](https://www.theanalysisfactor.com/why-report-estimated-marginal-means-in-spss-glm/), which enhances bare-bones descriptive statistics by accounting for imbalances in data. This is huge, because our mass data has double the `pbl` observations of the other categories. This method also helps with the imbalance from missing data, but that is a trivial concern compared to the double-PBL issue.

The second table output contains the pairwise contrasts between each level for a particular task, with the $\alpha$ = 0.05 p-value, associated 95% confidence interval, and Tukey family-wise adjustment.

## My Thoughts

This is messier than our previous output, but displays similar effects which lead to similar interpretations. It isn't nearly as parsimonious as "these 3 coefficients are negative, but PBL is the only positive one!" However, examine the `lec` contrasts and you can see ample comparisons which estimate `Remember` and `Understand` well below the higher levels, with significance. Conversely, these same comparisons have negative estimated effects for `pbl`, indicating a significant difference in the opposite direction. Middle Bloom's Levels are hazier to tease apart, as they were before, but you can see the clear stratification between `lec` `->` `hw/icp` `->` `pbl`, in my opinion.

# Kinetics Analysis

## Omnibus ANOVA

Here is the same procedure for kinetics. Starting with model fitting and the omnibus ANOVA:

```{r kinetics-lmm}
kinetics_lmm <- lmer(rating ~ blooms_level * task + (1 | year/student_id), data = kinetics)

anova(kinetics_lmm) %>%
  tidy()
```

## Estimated Marginal Means and Contrasts

``` {r kinetics-emmeans}
kinetics_emm <- ref_grid(kinetics_lmm) %>%
  emmeans(specs = pairwise ~ blooms_level | task)

tidy(kinetics_emm$emmeans)
kinetics_emm$contrasts %>%
  summary(infer = TRUE)
```

## My thoughts

Very analogous results again. `pbl` in particular **definitely** doesn't look as pretty as our original output.

But, I think we can interpret each of these significant pairwise comparisons in a much more statistically sound way, which will please reviewers.

# Assumptions for this mixed model

Final points: assumptions. This [paper](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13434) points out that even blatant violations are mostly fine, with the caveats of (1) no bimodial distributions and (2) noticeable skew and heteroscedasticity (which we have much of if I remember correctly) can lead to imprecise estimates. This is reflected in many wide confidence intervals in our data, and is an effect of both potential underlying skew/heteroscedasticity, but also more simply the Likert nature of our data.

I'll have more robust assumptions work added soon, but this is a great initial proof of concept and starting point for revisions.