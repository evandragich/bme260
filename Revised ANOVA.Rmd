---
title: "Revised Regression"
output:
  html_document:
    df_print: paged
---

# Data Cleaning

```{r load-packages-data, message = FALSE}
library(readxl) # tidy import of xlsx
library(tidyverse) # dplyr, magrittr
library(broom) # tidy
library(lme4) # lmer() (linear mixed models)
library(lmerTest) # lmer() anova
library(emmeans) # lmer() post hoc comparisons


# clean individual datasets into tidy format
mass_2019 <- read_excel(here::here("data_mass_only_from_2019_folder.xlsx"),
  sheet = "mass balances"
) %>%
  dplyr::select(-c(...2, ...3)) %>%
  drop_na(...1) %>%
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 12))) %>%
  rename(blooms_level = `...1`) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(
    year = "2019",
    student_id = str_to_lower(student_id)
  ) # 2019 student ids will be lowercase; 2020 uppercase

kinetics_2019 <- read_excel(here::here("data_kinetics_from_2019_folder.xlsx"),
  sheet = "kinetics"
) %>%
  dplyr::select(-c(...2, ...3)) %>%
  drop_na(...1) %>%
  slice_head(n = 24) %>% # remove irrelevant PBL1 data
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 6))) %>%
  rename(blooms_level = `...1`) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(
    year = "2019",
    student_id = str_to_lower(student_id)
  )

# 2020 didnt come with student id letter codes, so I'm making them here
# same sequence as 2019, but all caps to distinguish
helper_letters <- c(LETTERS, paste0(LETTERS, LETTERS)) %>%
  .[1:48]

mass_2020 <- read_excel(here::here("combined_2020.xlsx"),
  sheet = "Mass"
)

names(mass_2020) <- c("blooms_level", helper_letters)

mass_2020 <- mass_2020 %>%
  drop_na(A) %>%
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 12))) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(year = "2020",
         blooms_level = str_to_lower(blooms_level))

kinetics_2020 <- read_excel(here::here("combined_2020.xlsx"),
  sheet = "Kinetics"
)

names(kinetics_2020) <- c("blooms_level", helper_letters)

kinetics_2020 <- kinetics_2020 %>%
  drop_na(A) %>%
  slice(c(1:18, 25:30)) %>% # remove irrelevant PBL1 data
  mutate(task = c(rep("lec", 6), rep("icp", 6), rep("hw", 6), rep("pbl", 6))) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "student_id",
    values_to = "rating"
  ) %>%
  mutate(year = "2020",
         blooms_level = str_to_lower(blooms_level))

```

```{r combine-years}
# create final dataframes for each domain
mass <- rbind(mass_2019, mass_2020) %>%
  mutate(
    student_id = as.factor(student_id),
    year = factor(year),
    blooms_level = factor(blooms_level, levels = c("remember", "understand", "analyze", "apply", "evaluate", "create")),
    task = factor(task, levels = c("lec", "icp", "hw", "pbl"))
  )

# # gather student_ids of any students with NAs (ANOVA wont tolerate this).
# mass_to_remove <- mass %>%
#   filter(is.na(rating)) %>%
#   dplyr::select(student_id) %>%
#   pull()
# 
# # filter out all observations from said student ids
# mass <- mass %>%
#   filter(!(student_id %in% mass_to_remove)) %>%
#   mutate(student_id = fct_drop(student_id))

kinetics <- rbind(kinetics_2019, kinetics_2020) %>%
  mutate(
    student_id = factor(student_id),
    year = factor(year),
    blooms_level = factor(blooms_level, levels = c("remember", "understand", "analyze", "apply", "evaluate", "create")),
    task = factor(task, levels = c("lec", "icp", "hw", "pbl"))
  )
```

# Motivation for a new statistical method

My personal biggest concern with our methods, even before receiving reviewer comments, was the blatant violation of the [independence condition](https://statistics.laerd.com/spss-tutorials/multiple-regression-using-spss-statistics.php) for standard linear regression. I *knew* there was an alternative--something that could account for the repeated measures, or "nesting" of `blooms_level * task` within each student.

During my search, I finally found it-- [a two-way repeated measures ANOVA](https://statistics.laerd.com/spss-tutorials/two-way-repeated-measures-anova-using-spss-statistics.php). This page has good vignettes for sample scenarios, which you can draw analogies to our data structure.

I tried to find out how to implement it, and ran into [recent literature](https://www.r-bloggers.com/2018/04/how-to-do-repeated-measures-anovas-in-r/) eschewing ANOVA in favor of regression. This page also pointed me to the family of linear mixed models, which extend simple linear regression by allowing for random effects, in addtion to fixed.

Random effects control for the repeated measures factor, by essentially allowing factors to vary along `student_id` without using up degrees of freedom to try to make sense of the results. In other words, `student_id` is a random effect in our case because we have to account for each student having a different, random intercept, before examining the global, fixed effects of `task` and `blooms_level`.

The `(1 | year/student_id)` term in these models indicates that we have to account for the random, nuisance variance caused by each student and each year/cohort before examining the effects of `blooms_level`, `task`, and their interaction. Specifically, the `/` nesting operator means that we have a first random intercept to account for variance across years, and after accounting for this variance we introduce a random intercept to further account for that particular student's variance.


```{r mass-lmm}
mass_lmm <- lmer(rating ~ blooms_level * task + (1 | year/student_id), data = mass)


anova(mass_lmm) %>%
  tidy()
```

Great! Significant main effects for `task` and `blooms_level`, as well as their interaction. The next step is to break this down into pairwise comparisons. I chose to analyze pairs of `blooms_level` within each `task`; this is most analogous to our former method.

``` {r mass-emmeans}
#emmeans(mass_lmm, pairwise ~ blooms_level | task)

mass_emm <- ref_grid(mass_lmm) %>%
  emmeans(specs = pairwise ~ blooms_level | task)

tidy(mass_emm$emmeans)
mass_emm$contrasts %>%
  summary(infer = TRUE)
```

(Surprisingly, there don't exist any simple packages to transform this output into a neater table--I think we'd have to do it ourselves.)

The first table is the [marginal means](https://www.theanalysisfactor.com/why-report-estimated-marginal-means-in-spss-glm/), which enhances bare-bones descriptive statistics by accounting for imbalances in data. This is huge, because our mass data has double the `pbl` observations of the other categories. This method also helps with the imbalance from missing data, but that is a trivial concern compared to the double-PBL issue.

The second table output contains the pairwise contrasts between each level for a particular task, with the $\alpha$ = 0.05 p-value, associated 95% confidence interval, and Tukey familywise adjustment.

This is messier than our previous output, but displays similar effects which lead to similar interpretations. It isn't nearly as parsimonious as "these 3 coefficients are negative, but PBL is the only positive one!" However, examine the `lec` contrasts and you can see ample compr


```{r kinetics-lmm}
kinetics_lmm <- lmer(rating ~ blooms_level * task + (1 | year/student_id), data = kinetics)

anova(kinetics_lmm)
summary(kinetics_lmm)$coefficients
```